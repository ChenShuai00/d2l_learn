{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d457da35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(5.), tensor(6.), tensor(1.5000), tensor(9.))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#标量：仅包含一个数值\n",
    "import torch\n",
    "x = torch.tensor(3.0)\n",
    "y = torch.tensor(2.0)\n",
    "x + y, x * y, x / y, x**y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4161137e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我是向量 tensor([0, 1, 2, 3])\n",
      "x的长度: 4\n",
      "x的形状 torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "#向量：可视为标量值组成的列表。这些标量值被称为向量的元素或分量 粗体小写符号\n",
    "# 一般默认为列向量\n",
    "x = torch.arange(4)\n",
    "print('我是向量',x)\n",
    "#向量的长度，即其中元素的数量\n",
    "print('x的长度:',len(x))\n",
    "#。形状（shape）是一个元素组，列出了张量沿每个轴的长度（维数）。\n",
    "print('x的形状',x.shape)\n",
    "#向量或轴的维度表示向量或轴的长度，即向量或轴的元素数量\n",
    "#张量的维度用来表示张量具有的轴数。在这个意义上，张量的某个轴的维数就是这个轴的长度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cef7b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我是矩阵 tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11],\n",
      "        [12, 13, 14, 15],\n",
      "        [16, 17, 18, 19]])\n",
      "我是A的转置 tensor([[ 0,  4,  8, 12, 16],\n",
      "        [ 1,  5,  9, 13, 17],\n",
      "        [ 2,  6, 10, 14, 18],\n",
      "        [ 3,  7, 11, 15, 19]])\n"
     ]
    }
   ],
   "source": [
    "#矩阵：具有两个轴的张量，通常用粗体、大写字母来表示\n",
    "A = torch.arange(20).reshape(5, 4)\n",
    "print('我是矩阵',A)\n",
    "print('我是A的转置',A.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e924892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15.],\n",
      "        [16., 17., 18., 19.]]) tensor([[ 0.,  2.,  4.,  6.],\n",
      "        [ 8., 10., 12., 14.],\n",
      "        [16., 18., 20., 22.],\n",
      "        [24., 26., 28., 30.],\n",
      "        [32., 34., 36., 38.]]) tensor([[  0.,   1.,   4.,   9.],\n",
      "        [ 16.,  25.,  36.,  49.],\n",
      "        [ 64.,  81., 100., 121.],\n",
      "        [144., 169., 196., 225.],\n",
      "        [256., 289., 324., 361.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 2,  3,  4,  5],\n",
       "          [ 6,  7,  8,  9],\n",
       "          [10, 11, 12, 13]],\n",
       " \n",
       "         [[14, 15, 16, 17],\n",
       "          [18, 19, 20, 21],\n",
       "          [22, 23, 24, 25]]]),\n",
       " torch.Size([2, 3, 4]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#张量算法的基本性质\n",
    "#1.执行元素上的运算\n",
    "A = torch.arange(20, dtype=torch.float32).reshape(5, 4)\n",
    "B = A.clone() # 通过分配新内存，将A的一个副本分配给B\n",
    "print(A, A + B, A * B)\n",
    "#将张量乘以或加上一个标量不会改变张量的形状，其中张量的每个元素都将与标量相加或相乘\n",
    "a = 2\n",
    "X = torch.arange(24).reshape(2, 3, 4)\n",
    "a + X, (a * X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44370f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15.],\n",
      "        [16., 17., 18., 19.]]) tensor([[ 0.,  2.,  4.,  6.],\n",
      "        [ 8., 10., 12., 14.],\n",
      "        [16., 18., 20., 22.],\n",
      "        [24., 26., 28., 30.],\n",
      "        [32., 34., 36., 38.]]) tensor([[  0.,   1.,   4.,   9.],\n",
      "        [ 16.,  25.,  36.,  49.],\n",
      "        [ 64.,  81., 100., 121.],\n",
      "        [144., 169., 196., 225.],\n",
      "        [256., 289., 324., 361.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 2,  3,  4,  5],\n",
       "          [ 6,  7,  8,  9],\n",
       "          [10, 11, 12, 13]],\n",
       " \n",
       "         [[14, 15, 16, 17],\n",
       "          [18, 19, 20, 21],\n",
       "          [22, 23, 24, 25]]]),\n",
       " torch.Size([2, 3, 4]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#张量算法的基本性质\n",
    "#1.执行元素上的运算\n",
    "A = torch.arange(20, dtype=torch.float32).reshape(5, 4)\n",
    "B = A.clone() # 通过分配新内存，将A的一个副本分配给B\n",
    "print(A, A + B, A * B)\n",
    "#将张量乘以或加上一个标量不会改变张量的形状，其中张量的每个元素都将与标量相加或相乘\n",
    "a = 2\n",
    "X = torch.arange(24).reshape(2, 3, 4)\n",
    "a + X, (a * X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ac81dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我是A\n",
      " tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15.],\n",
      "        [16., 17., 18., 19.]])\n",
      "我的形状： torch.Size([5, 4])\n",
      "降维求和 tensor(190.)\n",
      "沿0轴或y轴求和 tensor([40., 45., 50., 55.])\n",
      "沿1轴或x轴求和 tensor([ 6., 22., 38., 54., 70.])\n",
      "获取张量内元素的个数： 20\n",
      "非降维求和\n",
      " tensor([[ 6.],\n",
      "        [22.],\n",
      "        [38.],\n",
      "        [54.],\n",
      "        [70.]])\n",
      "想沿0轴计算A元素的累积总和\n",
      " tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  6.,  8., 10.],\n",
      "        [12., 15., 18., 21.],\n",
      "        [24., 28., 32., 36.],\n",
      "        [40., 45., 50., 55.]])\n"
     ]
    }
   ],
   "source": [
    "#降维\n",
    "print(\"我是A\\n\",A)\n",
    "print('我的形状：',A.shape)\n",
    "print('降维求和',A.sum())\n",
    "print('沿0轴或y轴求和',A.sum(axis=0))\n",
    "print('沿1轴或x轴求和',A.sum(axis=1))\n",
    "print('获取张量内元素的个数：',A.numel())\n",
    "#非降维求和\n",
    "print('非降维求和\\n',A.sum(axis=1, keepdims=True))\n",
    "print('想沿0轴计算A元素的累积总和\\n',A.cumsum(axis=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6d89be88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 4]), torch.Size([4]), tensor([ 14,  38,  62,  86, 110]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 矩阵-向量积\n",
    "x = torch.arange(4)\n",
    "A = torch.arange(20).reshape(5, 4)\n",
    "A.shape, x.shape, torch.mv(A, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7046f652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我是A\n",
      " tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15.],\n",
      "        [16., 17., 18., 19.]])\n",
      "我的形状： torch.Size([5, 4])\n",
      "降维求和 tensor(190.)\n",
      "沿0轴或y轴求和 tensor([40., 45., 50., 55.])\n",
      "沿1轴或x轴求和 tensor([ 6., 22., 38., 54., 70.])\n",
      "获取张量内元素的个数： 20\n",
      "非降维求和\n",
      " tensor([[ 6.],\n",
      "        [22.],\n",
      "        [38.],\n",
      "        [54.],\n",
      "        [70.]])\n",
      "想沿0轴计算A元素的累积总和\n",
      " tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  6.,  8., 10.],\n",
      "        [12., 15., 18., 21.],\n",
      "        [24., 28., 32., 36.],\n",
      "        [40., 45., 50., 55.]])\n"
     ]
    }
   ],
   "source": [
    "#降维\n",
    "print(\"我是A\\n\",A)\n",
    "print('我的形状：',A.shape)\n",
    "print('降维求和',A.sum())\n",
    "print('沿0轴或y轴求和',A.sum(axis=0))\n",
    "print('沿1轴或x轴求和',A.sum(axis=1))\n",
    "print('获取张量内元素的个数：',A.numel())\n",
    "#非降维求和\n",
    "print('非降维求和\\n',A.sum(axis=1, keepdims=True))\n",
    "print('想沿0轴计算A元素的累积总和\\n',A.cumsum(axis=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a9f0e75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我是A\n",
      " tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15.],\n",
      "        [16., 17., 18., 19.]])\n",
      "我的形状： torch.Size([5, 4])\n",
      "降维求和 tensor(190.)\n",
      "沿0轴或y轴求和 tensor([40., 45., 50., 55.])\n",
      "沿1轴或x轴求和 tensor([ 6., 22., 38., 54., 70.])\n",
      "获取张量内元素的个数： 20\n",
      "非降维求和\n",
      " tensor([[ 6.],\n",
      "        [22.],\n",
      "        [38.],\n",
      "        [54.],\n",
      "        [70.]])\n",
      "想沿0轴计算A元素的累积总和\n",
      " tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  6.,  8., 10.],\n",
      "        [12., 15., 18., 21.],\n",
      "        [24., 28., 32., 36.],\n",
      "        [40., 45., 50., 55.]])\n"
     ]
    }
   ],
   "source": [
    "#降维\n",
    "print(\"我是A\\n\",A)\n",
    "print('我的形状：',A.shape)\n",
    "print('降维求和',A.sum())\n",
    "print('沿0轴或y轴求和',A.sum(axis=0))\n",
    "print('沿1轴或x轴求和',A.sum(axis=1))\n",
    "print('获取张量内元素的个数：',A.numel())\n",
    "#非降维求和\n",
    "print('非降维求和\\n',A.sum(axis=1, keepdims=True))\n",
    "print('想沿0轴计算A元素的累积总和\\n',A.cumsum(axis=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "537f22ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我是A\n",
      " tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15.],\n",
      "        [16., 17., 18., 19.]])\n",
      "我的形状： torch.Size([5, 4])\n",
      "降维求和 tensor(190.)\n",
      "沿0轴或y轴求和 tensor([40., 45., 50., 55.])\n",
      "沿1轴或x轴求和 tensor([ 6., 22., 38., 54., 70.])\n",
      "获取张量内元素的个数： 20\n",
      "非降维求和\n",
      " tensor([[ 6.],\n",
      "        [22.],\n",
      "        [38.],\n",
      "        [54.],\n",
      "        [70.]])\n",
      "想沿0轴计算A元素的累积总和\n",
      " tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  6.,  8., 10.],\n",
      "        [12., 15., 18., 21.],\n",
      "        [24., 28., 32., 36.],\n",
      "        [40., 45., 50., 55.]])\n"
     ]
    }
   ],
   "source": [
    "#降维\n",
    "print(\"我是A\\n\",A)\n",
    "print('我的形状：',A.shape)\n",
    "print('降维求和',A.sum())\n",
    "print('沿0轴或y轴求和',A.sum(axis=0))\n",
    "print('沿1轴或x轴求和',A.sum(axis=1))\n",
    "print('获取张量内元素的个数：',A.numel())\n",
    "#非降维求和\n",
    "print('非降维求和\\n',A.sum(axis=1, keepdims=True))\n",
    "print('想沿0轴计算A元素的累积总和\\n',A.cumsum(axis=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e49dd107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我是A\n",
      " tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15.],\n",
      "        [16., 17., 18., 19.]])\n",
      "我的形状： torch.Size([5, 4])\n",
      "降维求和 tensor(190.)\n",
      "沿0轴或y轴求和 tensor([40., 45., 50., 55.])\n",
      "沿1轴或x轴求和 tensor([ 6., 22., 38., 54., 70.])\n",
      "获取张量内元素的个数： 20\n",
      "非降维求和\n",
      " tensor([[ 6.],\n",
      "        [22.],\n",
      "        [38.],\n",
      "        [54.],\n",
      "        [70.]])\n",
      "想沿0轴计算A元素的累积总和\n",
      " tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  6.,  8., 10.],\n",
      "        [12., 15., 18., 21.],\n",
      "        [24., 28., 32., 36.],\n",
      "        [40., 45., 50., 55.]])\n"
     ]
    }
   ],
   "source": [
    "#降维\n",
    "print(\"我是A\\n\",A)\n",
    "print('我的形状：',A.shape)\n",
    "print('降维求和',A.sum())\n",
    "print('沿0轴或y轴求和',A.sum(axis=0))\n",
    "print('沿1轴或x轴求和',A.sum(axis=1))\n",
    "print('获取张量内元素的个数：',A.numel())\n",
    "#非降维求和\n",
    "print('非降维求和\\n',A.sum(axis=1, keepdims=True))\n",
    "print('想沿0轴计算A元素的累积总和\\n',A.cumsum(axis=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c81e4718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我是A\n",
      " tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15.],\n",
      "        [16., 17., 18., 19.]])\n",
      "我的形状： torch.Size([5, 4])\n",
      "降维求和 tensor(190.)\n",
      "沿0轴或y轴求和 tensor([40., 45., 50., 55.])\n",
      "沿1轴或x轴求和 tensor([ 6., 22., 38., 54., 70.])\n",
      "获取张量内元素的个数： 20\n",
      "非降维求和\n",
      " tensor([[ 6.],\n",
      "        [22.],\n",
      "        [38.],\n",
      "        [54.],\n",
      "        [70.]])\n",
      "想沿0轴计算A元素的累积总和\n",
      " tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  6.,  8., 10.],\n",
      "        [12., 15., 18., 21.],\n",
      "        [24., 28., 32., 36.],\n",
      "        [40., 45., 50., 55.]])\n"
     ]
    }
   ],
   "source": [
    "#降维\n",
    "print(\"我是A\\n\",A)\n",
    "print('我的形状：',A.shape)\n",
    "print('降维求和',A.sum())\n",
    "print('沿0轴或y轴求和',A.sum(axis=0))\n",
    "print('沿1轴或x轴求和',A.sum(axis=1))\n",
    "print('获取张量内元素的个数：',A.numel())\n",
    "#非降维求和\n",
    "print('非降维求和\\n',A.sum(axis=1, keepdims=True))\n",
    "print('想沿0轴计算A元素的累积总和\\n',A.cumsum(axis=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a5e87812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我是A\n",
      " tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15.],\n",
      "        [16., 17., 18., 19.]])\n",
      "我的形状： torch.Size([5, 4])\n",
      "降维求和 tensor(190.)\n",
      "沿0轴或y轴求和 tensor([40., 45., 50., 55.])\n",
      "沿1轴或x轴求和 tensor([ 6., 22., 38., 54., 70.])\n",
      "获取张量内元素的个数： 20\n",
      "非降维求和\n",
      " tensor([[ 6.],\n",
      "        [22.],\n",
      "        [38.],\n",
      "        [54.],\n",
      "        [70.]])\n",
      "想沿0轴计算A元素的累积总和\n",
      " tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  6.,  8., 10.],\n",
      "        [12., 15., 18., 21.],\n",
      "        [24., 28., 32., 36.],\n",
      "        [40., 45., 50., 55.]])\n"
     ]
    }
   ],
   "source": [
    "#降维\n",
    "print(\"我是A\\n\",A)\n",
    "print('我的形状：',A.shape)\n",
    "print('降维求和',A.sum())\n",
    "print('沿0轴或y轴求和',A.sum(axis=0))\n",
    "print('沿1轴或x轴求和',A.sum(axis=1))\n",
    "print('获取张量内元素的个数：',A.numel())\n",
    "#非降维求和\n",
    "print('非降维求和\\n',A.sum(axis=1, keepdims=True))\n",
    "print('想沿0轴计算A元素的累积总和\\n',A.cumsum(axis=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9d4ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#范数(norm):向量的范数是表示一个向量有多大。这里考虑的大小（size）概念不涉及维度，而是分量的大小。\n",
    "#L2范数：向量元素平方和的平方根\n",
    "u = torch.tensor([3.0, -4.0])\n",
    "torch.norm(u)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
    "#L1范数：表示为向量元素的绝对值之和\n",
    "torch.abs(u).sum()\n",
    "#矩阵的Frobenius范数：是矩阵元素平方和的平方根\n",
    "torch.norm(torch.ones((4, 9)))\n",
    "#范数与目标函数息息相关，用向量表示物品（如单词、产品或新闻文章），以便最小化相似项目之间的距离，最大化不同项目之间的距离"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
